#!/bin/bash
#SBATCH --job-name=denv_bioinformatics_sample
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --partition=interactive
#SBATCH --account=default
#SBATCH --time=02:00:00
#SBATCH --mail-user snyathi@stanford.edu
#SBATCH --output=/labs/dlabeaud/snyathi_1/analytic_pipeline/Logs/denv_bioinf_single.%A.%a.out
#SBATCH --error=/labs/dlabeaud/snyathi_1/analytic_pipeline/Logs/denv_bioinf_single.%A.%a.err

# Sindiso Nyathi, Desiree LaBeaud, Jason Andrews, Shannon Bennet, Izabella Rezende, Panpim Thongsripon, Katie Walter, Renu Verma
# Dengue Evolution Project Analytic Pipeline.
# This script processes demultiplexed run out put data generated by base space, using a custom pipeline based on iVar (Grubaugh et al)
# The script reads in Read1.gzip and Read2.gzip files for N samples, and outputs various intermediate files.  
# Raw files are read in from a raw_runsets_x folder, where X is a group of raw runs for the given project of sequencing run. 

# ****************************************************************#
 

# ****************************************************************#
# Preliminaries

# Load required libraries/programs (fastqc, trimgalore, and bowtie)
module load miniconda
module load legacy/scg4
module load legacy/.base
module load java/8u66
module load python


# Quit if an error occurs. 
set -e

# Activate virtual environment so we can use ivar
source activate /labs/dlabeaud/snyathi_1/analytic_pipeline/Software

# Set the working directory. Working Directory is the main directory for LaBeaud Lab. 
cd /labs/dlabeaud/snyathi_1/analytic_pipeline/

# Load more modules
module load fastqc
module load cutadapt/1.16
module load trim_galore
module load bowtie2
module load samtools/1.17
module load bwa

# Set the sample ID and reference. 
# Retrieve the current sample ids and invidiaul read file ids.
# IDs
this_sample_id=$(awk -v "ArrayTaskID=$SLURM_ARRAY_TASK_ID" '{if (NR == ArrayTaskID) print $1}' raw_runsets/denv_cohort/denvcohort_sampleids.txt)
this_sample_sero=$(awk -v "ArrayTaskID=$SLURM_ARRAY_TASK_ID" '{if (NR == ArrayTaskID) print $1}' raw_runsets/denv_cohort/denvcohort_sampleseros.txt)

# Reads
read1=$(awk -v "ArrayTaskID=$SLURM_ARRAY_TASK_ID" '{if (NR == ArrayTaskID) print $1}' raw_runsets/denv_cohort/denvcohort_reads.txt)
read2=$(awk -v "ArrayTaskID=$SLURM_ARRAY_TASK_ID" '{if (NR == ArrayTaskID) print $2}' raw_runsets/denv_cohort/denvcohort_reads.txt)

# References
this_sample_ref=$(awk -v sample_sero=$this_sample_sero '{if (NR == sample_sero) print $1}' raw_runsets/denv_cohort/denv_refs.txt)
this_sample_bed=$(awk -v sample_sero=$this_sample_sero '{if (NR == sample_sero) print $1}' raw_runsets/denv_cohort/bed_files.txt)
# ****************************************************************#

# Complete a single analysis for each pair of reads in the file. 

# Step 0: Find read 1 and read II file names. 
# Note use of the $ to save the output of a command.
echo Sample Start Point
echo Declared Variables
echo $this_sample_id
echo $this_sample_sero
echo $read1
echo $read2
echo $this_sample_ref
echo $this_sample_bed

# Step 0 Create a temporary directory for temporary analysis files. 
mkdir temporary_analysis/$SLURM_ARRAY_TASK_ID

# Step 1: TrimGalore files analysis.

# Trim adapters and indexes of both read files. 
# Currently unsure if this step is necessary or if ivar trim duplicates this. 

# Run fastq.
# Status check. 
echo  "Running FastQC on $read1 and $read2 . . . "

fastqc raw_runsets/denv_cohort/$read1 --outdir=Results/denv_cohort
fastqc raw_runsets/denv_cohort/$read2 --outdir=Results/denv_cohort

# Run BWA_MEM for sequence alignment. 
echo "Running mapping with BWA_MEM on $read1 and $read2 . . .  "

# Run bwa men
echo "Running bwa mem on $this_sample_id . . . "
bwa mem $this_sample_ref raw_runsets/denv_cohort/$read1 raw_runsets/denv_cohort/$read2 | samtools sort > temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id".bam"

# Trim primers using ivar trim
echo "Running iVar trim on $this_sample_id . . . "
ivar trim -i temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id".bam" -b $this_sample_bed -q 20 -m 30 -p temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed.bam" -e

# Sort again
samtools sort temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed.bam" -o temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed_sorted.bam"

fastqc temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed_sorted.bam" --outdir=Results/denv_cohort

# Samtools mpileup and ivar consensus
echo "Mpileup and Consensus on $this_sample_id . . . "
samtools mpileup -A -d 0 -Q 20 --reference $this_sample_ref temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed_sorted.bam" | ivar consensus -p Results/denv_cohort/$this_sample_id -n N -q 20 -m 10 -t 0.8

# Coverage
echo "Coverage and Depth on $this_sample_id . . . "
samtools coverage temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed_sorted.bam" -o Results/denv_cohort/$this_sample_id"_coverage.txt"

# Depth
samtools depth -a -q 20 temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed_sorted.bam" -o Results/denv_cohort/$this_sample_id"_depth.txt"

# copy the bam and bai index file to the results folder.
echo "Cleaning up "
cp temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id"_trimmed_sorted.bam" Results/denv_cohort
cp temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id".bam.bai" Results/denv_cohort

# Remove the files from the intermediate directory associated with this file. 
rm temporary_analysis/$SLURM_ARRAY_TASK_ID/$this_sample_id*

echo "Sample_End_Point "

conda deactivate
